{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f0b45f-5fa2-405a-9b6d-a60173874913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73929faf-60d4-48c7-b1ba-23b6eeb70ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders=['male','female']\n",
    "X=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069c6ad-f043-40b8-8950-7b1170e39925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef4c179-01dc-4a83-8b59-073d6dd41d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gender in genders:\n",
    "    folder=(f\"d:/dataset/dataset/gender_training/train/{gender}/\")\n",
    "    images = list(paths.list_images(folder))\n",
    "    for img_path in images:\n",
    "        img_clr = cv2.imread(img_path)\n",
    "        img_clr = cv2.resize(img_clr, (60, 60))\n",
    "        img_gray = cv2.cvtColor(img_clr, cv2.COLOR_BGR2GRAY)\n",
    "        img_gray = img_gray / 255.0\n",
    "        X.append(img_gray.flatten())\n",
    "        y.append(gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cc3857-0f1e-423f-815e-6468494c1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec6b7db-5352-4834-81f4-c0e25ecf6548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf282711-adaf-4ae8-83ee-003cf7695f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_image=list(paths.list_images(\"d:/dataset/dataset/gender_training/test/female/female_10.jpg\"))\n",
    "print(len(male_image))\n",
    "for img in male_image:\n",
    "    img=cv2.imread(img)\n",
    "    img_clr=cv2.resize(img,(100,100))\n",
    "    img_gray=cv2.cvtColor(img_clr,cv2.COLOR_BGR2GRAY)\n",
    "    img_gray=img_gray/255\n",
    "    pred=model.predict([img_gray.flatten()])\n",
    "    print(pred)\n",
    "    print(model.predict_proba([img_gray.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f603368a-375d-4fce-a60e-89ab4029b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame\n",
    "        img_clr = cv2.resize(frame, (60, 60))\n",
    "        img_gray = cv2.cvtColor(img_clr, cv2.COLOR_BGR2GRAY)\n",
    "        img_gray = img_gray / 255.0\n",
    "        img_flatten = img_gray.flatten().reshape(1, -1)\n",
    "\n",
    "        # Predict the gender\n",
    "        prediction = model.predict(img_flatten)\n",
    "        gender = prediction[0]\n",
    "\n",
    "        # Display the gender on the frame\n",
    "        cv2.putText(frame, gender, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Gender Recognition', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1e5c8-365f-484c-856b-d20b075b963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_image=list(paths.list_images(\"d:/dataset/dataset/gender_training/train/male/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e8ebe-4fe5-4bdc-92f2-b1e091069590",
   "metadata": {},
   "outputs": [],
   "source": [
    "femal_image=list(paths.list_images(\"d:/dataset/dataset/gender_training/train/female/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98425729-fe57-484a-9b65-54bbd81211bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for img in male_image:\n",
    "    img_clr=cv2.imread(img)\n",
    "    img_clr=cv2.resize(img_clr,(100,100))\n",
    "    img_gray=cv2.cvtColor(img_clr,cv2.COLOR_BGR2GRAY)\n",
    "    img_gray=img_gray/255\n",
    "    X.append(img_gray.flatten())\n",
    "    y.append(\"male\")\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd5a0c-d064-464a-86f6-a520b51891d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in femal_image:\n",
    "    img_clr=cv2.imread(img)\n",
    "    img_clr=cv2.resize(img_clr,(100,100))\n",
    "    img_gray=cv2.cvtColor(img_clr,cv2.COLOR_BGR2GRAY)\n",
    "    img_gray=img_gray/255\n",
    "    X.append(img_gray.flatten())\n",
    "    y.append(\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13a067-8e4b-4777-b50b-81d0d34e0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(solver='saga',max_iter=10000)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f32f12-9d07-4d1a-987d-1ed5b3997a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo=cv2.VideoCapture(0)\n",
    "facemodel=cv2.CascadeClassifier(\"d:/dataset/dataset/haar/haarcascade_frontalface_default.xml\")\n",
    "while True:\n",
    "   flag,img= vdo.read()\n",
    "   if flag== False :\n",
    "       break\n",
    "   img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "   list_img=facemodel.detectMultiScale(img_gray,minNeighbors=8,scaleFactor=1.2)\n",
    "   for x,y,w,h in list_img:\n",
    "       cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "       face=img[y:y+h,x:x+h]\n",
    "       face2=cv2.resize(face,(100,100))\n",
    "       face_gray=cv2.cvtColor(face2,cv2.COLOR_BGR2GRAY)\n",
    "       face_gray=face_gray/255\n",
    "       pred=model.predict([img_gray.flatten()])\n",
    "       prob=model.predict_proba([img_gray.flatten()])\n",
    "       cv2.putText(img,pred[0],(x,y),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "       cv2.putText(img,\"%.2f\"%(np.max(prob[0])),(x,y+20),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "   cv2.imshow(\"rajat\",img)\n",
    "   key=cv2.waitKey(25)\n",
    "   if key==ord('q'):\n",
    "       break\n",
    "cv2.destroyAllWindows()\n",
    "vdo.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276fd94-9ba0-457b-b607-1bcae6165028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a138f74e-db0b-4521-8b1e-0bfba97a1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_image=list(paths.list_images(\"d:/dataset/dataset/gender_training/test/female/\"))\n",
    "print(len(male_image))\n",
    "for img in male_image:\n",
    "    img=cv2.imread(img)\n",
    "    img_clr=cv2.resize(img,(100,100))\n",
    "    img_gray=cv2.cvtColor(img_clr,cv2.COLOR_BGR2GRAY)\n",
    "    img_gray=img_gray/255\n",
    "    pred=model.predict([img_gray.flatten()])\n",
    "    print(pred)\n",
    "    print(model.predict_proba([img_gray.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28353c68-ad6e-45f6-b87c-c4fbf14d0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img=cv2.imread(\"d:/dataset/dataset/gender_training/test/male/male_1.jpg\")\n",
    "img_clr=cv2.resize(img,(100,100))\n",
    "img_gray=cv2.cvtColor(img_clr,cv2.COLOR_BGR2GRAY)\n",
    "img_gray=img_gray/255\n",
    "pred=model.predict([img_gray.flatten()])\n",
    "prob=model.predict_proba([img_gray.flatten()])\n",
    "cv2.putText(img,pred[0],(8,25),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "cv2.putText(img,\"%.2f\"%(np.max(prob[0])),(20,50),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce07c8c-a214-4e57-a23b-c8a21ea84c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "# Replace with the path to your pre-trained model\n",
    "\n",
    "# Initialize the video capture and face detection\n",
    "vdo = cv2.VideoCapture(0)\n",
    "facemodel = cv2.CascadeClassifier(\"d:/dataset/dataset/haar/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    flag, img = vdo.read()\n",
    "    if flag == False:\n",
    "        break\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    list_img = facemodel.detectMultiScale(img_gray, minNeighbors=8, scaleFactor=1.2)\n",
    "    \n",
    "    for x, y, w, h in list_img:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "        face = img[y:y + h, x:x + h]\n",
    "        face2 = cv2.resize(face, (100, 100))\n",
    "        face_gray = cv2.cvtColor(face2, cv2.COLOR_BGR2GRAY)\n",
    "        face_gray = face_gray / 255.0\n",
    "        \n",
    "        # Prepare the face image for prediction\n",
    "        face_input = face_gray.reshape(1, 100, 100, 1)  # Adjust according to your model input shape\n",
    "        pred = model.predict(face_input)\n",
    "        \n",
    "        # Assuming the model returns a binary classification: 0 for Female and 1 for Male\n",
    "        gender = 'Male' if pred[0][0] > 0.5 else 'Female'\n",
    "        prob = pred[0][0] if pred[0][0] > 0.5 else 1 - pred[0][0]\n",
    "        \n",
    "        cv2.putText(img, gender, (x, y), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "        cv2.putText(img, \"%.2f\" % (prob), (x, y + 20), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Gender Prediction\", img)\n",
    "    key = cv2.waitKey(25)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vdo.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3a5f9-7f76-434d-9bac-91475c258b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
